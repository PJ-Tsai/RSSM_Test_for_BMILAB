{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee535e-59b8-4375-83d0-dc009340efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSSM-based Delta Pose Predictor for Ultrasound Images\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3070b1-33c4-436e-852e-ea3ee01462f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Data Root\n",
    "data_root = '.'  # /path/to/data\n",
    "\n",
    "# Set train/test folder names\n",
    "train_dirs = {\n",
    "    \"frames_0513_06\", \"frames_0513_07\", \"frames_0513_08\", \"frames_0513_09\", \"frames_0513_10\",\n",
    "    \"frames_0513_11\", \"frames_0513_12\", \"frames_0513_13\", \"frames_0513_14\", \"frames_0513_15\",\n",
    "    \"frames_0513_16\", \"frames_0513_17\", \"frames_0513_18\", \"frames_0513_19\", \"frames_0513_20\",\n",
    "    \"frames_0513_21\", \"frames_0513_22\", \"frames_0513_23\", \"frames_0513_24\", \"frames_0513_25\", \"frames_0513_26\"\n",
    "}\n",
    "\n",
    "test_dirs = {\n",
    "    \"frames_0513_01\", \"frames_0513_02\", \"frames_0513_03\", \"frames_0513_04\", \"frames_0513_05\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617969d-12ba-4211-81a2-9ae27de1769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Combine CSVs ============\n",
    "def combine_pose_csvs_with_foldername(root_folder, output_csv=\"poses_combined.csv\"):\n",
    "    all_data = []\n",
    "\n",
    "    for file in sorted(os.listdir(root_folder)):\n",
    "        if not file.endswith(\"_final_data.csv\"):\n",
    "            continue\n",
    "\n",
    "        csv_path = os.path.join(root_folder, file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # ex: 0513_01_final_data.csv → frames_0513_01\n",
    "        folder_name = \"frames_\" + file.replace(\"_final_data.csv\", \"\")\n",
    "\n",
    "        # Update Filename Column: → frames_0513_01/frame_0000.png\n",
    "        df[\"Filename\"] = df[\"Filename\"].apply(lambda x: f\"{folder_name}/{x}\")\n",
    "        all_data.append(df)\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"⚠️ No Valid File Found\")\n",
    "        return\n",
    "\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Saved Combined CSV to：{output_csv}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02618978-809d-42b2-b1dc-6a74c16aec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_pose_csvs_with_foldername(data_root, \"poses_combined.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a15389-93de-438a-91e2-1ee7c5a83e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dataset ===\n",
    "class SequenceUltrasoundDataset(Dataset):\n",
    "    def __init__(self, csv_path, root_dirs, sequence_length=5, image_size=(256, 256)):\n",
    "        self.samples = []\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        df['folder'] = df['Filename'].apply(lambda x: x.split('/')[0])\n",
    "        df = df.rename(columns={\n",
    "            'Filename': 'img_path',\n",
    "            'X (mm)': 'tx', 'Y (mm)': 'ty', 'Z (mm)': 'tz',\n",
    "            'Roll (deg)': 'rx', 'Pitch (deg)': 'ry', 'Yaw (deg)': 'rz'\n",
    "        })\n",
    "\n",
    "        for dir_ in root_dirs:\n",
    "            group = df[df['folder'] == dir_].sort_values('img_path')\n",
    "            frames = group.to_dict('records')\n",
    "            for i in range(len(frames) - sequence_length):\n",
    "                seq = frames[i:i + sequence_length + 1]\n",
    "                self.samples.append(seq)\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.image_size = image_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.samples[idx]\n",
    "        imgs, poses, delta_poses = [], [], []\n",
    "        for item in seq:\n",
    "            img_path = item['img_path']\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Cannot read image: {img_path}\")\n",
    "            img = self.transform(img)\n",
    "            pose = np.array([\n",
    "                item['tx'], item['ty'], item['tz'],\n",
    "                item['rx'], item['ry'], item['rz']\n",
    "            ], dtype=np.float32)\n",
    "            imgs.append(img)\n",
    "            poses.append(pose)\n",
    "\n",
    "        poses = torch.tensor(np.array(poses), dtype=torch.float32)\n",
    "        delta_poses = poses[1:] - poses[:-1]\n",
    "\n",
    "        return (\n",
    "            torch.stack(imgs[:-1]),       # [T, 1, H, W]\n",
    "            delta_poses                  # [T, 6]\n",
    "        )\n",
    "\n",
    "# === Encoder ===\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, stride=2, padding=1), nn.InstanceNorm2d(32), nn.ReLU(),  # 256→128\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1), nn.InstanceNorm2d(64), nn.ReLU(),  # 128→64\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1), nn.InstanceNorm2d(128), nn.ReLU(),  # 64→32\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1), nn.InstanceNorm2d(256), nn.ReLU()  # 32→16\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(256 * 16 * 16, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 16 * 16, latent_dim)\n",
    "        # self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z, mu, logvar\n",
    "\n",
    "# === RSSM Core ===\n",
    "class RSSMCore(nn.Module):\n",
    "    def __init__(self, action_dim, z_dim, h_dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "        self.project_action_z = nn.Linear(z_dim + action_dim, h_dim)\n",
    "        self.gru = nn.GRUCell(h_dim, h_dim)\n",
    "\n",
    "        self.project_hidden_action = nn.Linear(h_dim + action_dim, h_dim)\n",
    "        self.prior = nn.Linear(h_dim, z_dim * 2)\n",
    "\n",
    "        self.project_hidden_obs = nn.Linear(h_dim + embed_dim, h_dim)\n",
    "        self.posterior = nn.Linear(h_dim, z_dim * 2)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, prev_z, prev_h, actions, embeddings=None, dones=None):\n",
    "        B, T, _ = actions.size()\n",
    "        h, z = prev_h, prev_z\n",
    "\n",
    "        h_seq, z_seq, prior_mean_seq, prior_std_seq = [], [], [], []\n",
    "        post_mean_seq, post_std_seq = [], []\n",
    "\n",
    "        for t in range(T):\n",
    "            a = actions[:, t]\n",
    "            e = embeddings[:, t] if embeddings is not None else None\n",
    "\n",
    "            # Reset z if done\n",
    "            if dones is not None:\n",
    "                z = z * (1.0 - dones[:, t])\n",
    "\n",
    "            x = torch.cat([z, a], dim=-1)\n",
    "            x = self.activation(self.project_action_z(x))\n",
    "            h = self.gru(x, h)\n",
    "\n",
    "            # Prior\n",
    "            ha = torch.cat([h, a], dim=-1)\n",
    "            ha = self.activation(self.project_hidden_action(ha))\n",
    "            prior_params = self.prior(ha)\n",
    "            prior_mean, prior_logstd = torch.chunk(prior_params, 2, dim=-1)\n",
    "            prior_std = F.softplus(prior_logstd)\n",
    "            prior_dist = torch.distributions.Normal(prior_mean, prior_std)\n",
    "            prior_z = prior_dist.rsample()\n",
    "\n",
    "            # Posterior\n",
    "            if embeddings is not None:\n",
    "                he = torch.cat([h, e], dim=-1)\n",
    "                he = self.activation(self.project_hidden_obs(he))\n",
    "                post_params = self.posterior(he)\n",
    "                post_mean, post_logstd = torch.chunk(post_params, 2, dim=-1)\n",
    "                post_std = F.softplus(post_logstd)\n",
    "                post_dist = torch.distributions.Normal(post_mean, post_std)\n",
    "                post_z = post_dist.rsample()\n",
    "            else:\n",
    "                post_z = prior_z\n",
    "                post_mean, post_std = prior_mean, prior_std\n",
    "\n",
    "            z = post_z\n",
    "\n",
    "            # Collect for each timestep\n",
    "            h_seq.append(h.unsqueeze(1))\n",
    "            z_seq.append(z.unsqueeze(1))\n",
    "            prior_mean_seq.append(prior_mean.unsqueeze(1))\n",
    "            prior_std_seq.append(prior_std.unsqueeze(1))\n",
    "            post_mean_seq.append(post_mean.unsqueeze(1))\n",
    "            post_std_seq.append(post_std.unsqueeze(1))\n",
    "\n",
    "        return {\n",
    "            'h': torch.cat(h_seq, dim=1),\n",
    "            'z': torch.cat(z_seq, dim=1),\n",
    "            'prior_mean': torch.cat(prior_mean_seq, dim=1),\n",
    "            'prior_std': torch.cat(prior_std_seq, dim=1),\n",
    "            'post_mean': torch.cat(post_mean_seq, dim=1),\n",
    "            'post_std': torch.cat(post_std_seq, dim=1),\n",
    "        }\n",
    "\n",
    "# === Pose Decoder ===\n",
    "class PoseDecoder(nn.Module):\n",
    "    def __init__(self, h_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(h_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        return self.fc(h)\n",
    "\n",
    "# === Frame Decoder ===\n",
    "class FrameDecoder(nn.Module):\n",
    "    def __init__(self, h_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(h_dim, 128 * 16 * 16)\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 4, 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        x = self.fc(h).view(-1, 128, 16, 16)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "# === RSSM Model ===\n",
    "class RSSMGoalDeltaPoseModel(nn.Module):\n",
    "    def __init__(self, z_dim=64, h_dim=128, action_dim=6, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.rssm = RSSMCore(action_dim, z_dim, h_dim, embed_dim)\n",
    "        self.pose_decoder = PoseDecoder(h_dim)\n",
    "        self.frame_decoder = FrameDecoder(h_dim)\n",
    "        self.z_dim = z_dim\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "    def forward(self, obs_seq, delta_pose_seq):  # obs_seq: [B, T, 1, H, W], delta_pose_seq: [B, T, 6]\n",
    "        B, T, C, H, W = obs_seq.shape\n",
    "        device = obs_seq.device\n",
    "        h = torch.zeros(B, self.h_dim, device=device)\n",
    "        z = torch.zeros(B, self.z_dim, device=device)\n",
    "\n",
    "        embeddings, mus, logvars = [], [], []\n",
    "        for t in range(T):\n",
    "            z_t, mu_t, logvar_t = self.encoder(obs_seq[:, t])\n",
    "            embeddings.append(z_t.unsqueeze(1))\n",
    "            mus.append(mu_t.unsqueeze(1))\n",
    "            logvars.append(logvar_t.unsqueeze(1))\n",
    "        embeddings = torch.cat(embeddings, dim=1)  # [B, T, z_dim]\n",
    "        mus = torch.cat(mus, dim=1)\n",
    "        logvars = torch.cat(logvars, dim=1)\n",
    "\n",
    "        rssm_out = self.rssm(z, h, delta_pose_seq, embeddings=embeddings)\n",
    "\n",
    "        pose_preds = self.pose_decoder(rssm_out['h'])  # [B, T, 6]\n",
    "        img_preds = self.frame_decoder(rssm_out['h'].reshape(-1, self.h_dim)).reshape(B, T, 1, H, W)\n",
    "\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvars - mus.pow(2) - logvars.exp(), dim=-1).mean()\n",
    "        return pose_preds, img_preds, kl_loss\n",
    "\n",
    "# === Train Model ===\n",
    "def train_model(csv_path, train_dirs, test_dirs, image_size=(256, 256), batch_size=32, epochs=20, lr=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_dataset = SequenceUltrasoundDataset(csv_path, train_dirs, sequence_length=10, image_size=image_size)\n",
    "    test_dataset = SequenceUltrasoundDataset(csv_path, test_dirs, sequence_length=10, image_size=image_size)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = RSSMGoalDeltaPoseModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ==== Training ====\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for obs_seq, delta_poses_seq in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "            obs_seq = obs_seq.to(device)\n",
    "            delta_poses_seq = delta_poses_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pose_preds, img_preds, kl_loss = model(obs_seq, delta_poses_seq)\n",
    "\n",
    "            loss_pose = F.smooth_l1_loss(pose_preds, delta_poses_seq)\n",
    "            loss_recon = F.mse_loss(img_preds, obs_seq)\n",
    "            loss = 2.0 * loss_pose + 0.5 * loss_recon + 0.1 * kl_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # ==== Testing ====\n",
    "        model.eval()\n",
    "        total_test_loss = 0.0\n",
    "        all_test_pred = []\n",
    "        all_test_true = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for obs_seq, delta_poses_seq in tqdm(test_loader, desc=f\"Epoch {epoch+1} Testing\"):\n",
    "                obs_seq = obs_seq.to(device)\n",
    "                delta_poses_seq = delta_poses_seq.to(device)\n",
    "\n",
    "                pose_preds, img_preds, kl_loss = model(obs_seq, delta_poses_seq)\n",
    "\n",
    "                loss_pose = F.smooth_l1_loss(pose_preds, delta_poses_seq)\n",
    "                loss_recon = F.mse_loss(img_preds, obs_seq)\n",
    "                loss = 5.0 * loss_pose + 0.5 * loss_recon + 0.1 * kl_loss\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "                all_test_pred.append(pose_preds.cpu().numpy())\n",
    "                all_test_true.append(delta_poses_seq.cpu().numpy())\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_loader)\n",
    "\n",
    "        # ==== Save test predictions and ground truth as CSV ====\n",
    "        all_test_pred_arr = np.concatenate(all_test_pred, axis=0)  # [num_batches*B, T, 6]\n",
    "        all_test_true_arr = np.concatenate(all_test_true, axis=0)\n",
    "\n",
    "        all_test_pred_flat = all_test_pred_arr.reshape(-1, 6)\n",
    "        all_test_true_flat = all_test_true_arr.reshape(-1, 6)\n",
    "\n",
    "        df_test_results = pd.DataFrame({\n",
    "            'pred_tx': all_test_pred_flat[:, 0],\n",
    "            'pred_ty': all_test_pred_flat[:, 1],\n",
    "            'pred_tz': all_test_pred_flat[:, 2],\n",
    "            'pred_rx': all_test_pred_flat[:, 3],\n",
    "            'pred_ry': all_test_pred_flat[:, 4],\n",
    "            'pred_rz': all_test_pred_flat[:, 5],\n",
    "            'true_tx': all_test_true_flat[:, 0],\n",
    "            'true_ty': all_test_true_flat[:, 1],\n",
    "            'true_tz': all_test_true_flat[:, 2],\n",
    "            'true_rx': all_test_true_flat[:, 3],\n",
    "            'true_ry': all_test_true_flat[:, 4],\n",
    "            'true_rz': all_test_true_flat[:, 5],\n",
    "        })\n",
    "        # === Accuracy Calculation for each of the 6 components ===\n",
    "        epsilon = 1e-6  \n",
    "\n",
    "        # flatten prediction and ground truth: [N, 6]\n",
    "        pred = all_test_pred_flat\n",
    "        true = all_test_true_flat\n",
    "\n",
    "        # Bound: 50% ~ 150% of Ground Truth \n",
    "        within_bounds = (pred >= 0.5 * (true + epsilon)) & (pred <= 1.5 * (true + epsilon))\n",
    "\n",
    "        true_zero = np.abs(true) < epsilon\n",
    "        pred_zero = np.abs(pred) < epsilon\n",
    "        zero_match = true_zero & pred_zero\n",
    "\n",
    "        correct_mask = within_bounds | zero_match  # shape [N, 6]\n",
    "\n",
    "        component_names = ['tx', 'ty', 'tz', 'rx', 'ry', 'rz']\n",
    "        component_accuracies = {\n",
    "            name: correct_mask[:, i].mean()\n",
    "            for i, name in enumerate(component_names)\n",
    "        }\n",
    "\n",
    "        csv_filename = f'new_test_pred_true_epoch_{epoch+1}.csv'\n",
    "        df_test_results.to_csv(csv_filename, index=False)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
    "        print(f\"Per-component Accuracy (within 50%~150% of GT):\")\n",
    "        for name in component_names:\n",
    "            print(f\"  {name}: {component_accuracies[name]:.4f}\")\n",
    "        print(f\"Test predictions and true values saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04086aa2-9e59-44e5-a38c-1e6ea4c30a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run ===\n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        csv_path=\"poses_combined.csv\",\n",
    "        train_dirs=train_dirs,\n",
    "        test_dirs=test_dirs,\n",
    "        epochs=20,\n",
    "        batch_size=64,\n",
    "        lr=1e-4,\n",
    "        image_size=(256, 256)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc736754-fbce-4172-ba26-1b47777473a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Predict ===\n",
    "def predict_action(current_img, goal_img, model_path=\"rssm_model.pth\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = RSSMGoalDeltaPoseModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    current_img = transform(current_img).unsqueeze(0).to(device)  # (1, 1, H, W)\n",
    "    goal_img = transform(goal_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        delta_pose, _ = model(current_img, goal_img)\n",
    "    \n",
    "    return delta_pose.squeeze(0).cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
